{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 基本矩阵运算\n",
    "import math\n",
    "# 基本数学运算\n",
    "import matplotlib.pyplot as plt\n",
    "# 图片绘制\n",
    "import h5py\n",
    "# 数据集读取\n",
    "import copy\n",
    "# 深浅拷贝"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图片绘制\n",
    "\n",
    "def paint(X):\n",
    "    plt.figure()\n",
    "    #创建画布\n",
    "    plt.imshow(X,cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图片格式转化\n",
    "\n",
    "def dataset_rgb_to_gray(X):\n",
    "    size = len(X)\n",
    "    width = len(X[0])\n",
    "    height = len(X[0][0])\n",
    "    X_tmp = np.zeros([size,width,height])\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X[i])):\n",
    "            for k in range(len(X[i][j])):\n",
    "                r = X[i][j][k][0]\n",
    "                g = X[i][j][k][1]\n",
    "                b = X[i][j][k][2]\n",
    "                X_tmp[i][j][k] = np.array(r*0.299 + g*0.587 + b*0.114)\n",
    "    return X_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数定义\n",
    "def null(x):\n",
    "    return x\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_back(x):\n",
    "    dx = x\n",
    "    dx[x<=0] = 0\n",
    "    dx[x>0] = 1\n",
    "    return dx\n",
    "\n",
    "def sigmoid(x):    \n",
    "    return 1.0/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_back(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "\n",
    "X_train=np.array(train_dataset[\"train_set_x\"][:])\n",
    "Y_train=np.array(train_dataset[\"train_set_y\"][:])\n",
    "\n",
    "X_test=np.array(test_dataset[\"test_set_x\"][:])\n",
    "Y_test=np.array(test_dataset[\"test_set_y\"][:])\n",
    "\n",
    "# 图片通道转换\n",
    "X_train = dataset_rgb_to_gray(X_train)\n",
    "X_test = dataset_rgb_to_gray(X_test)\n",
    "\n",
    "# 图片一维化\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "train_num = len(X_train)\n",
    "test_num = len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数初始化\n",
    "\n",
    "def init_parameters(dimensions):\n",
    "    parameters = {}\n",
    "    layer_num = len(dimensions)\n",
    "    for index in range(layer_num):\n",
    "        if index == 0:\n",
    "            parameters['b0'] = np.zeros(np.array(dimensions[0]))\n",
    "        else:\n",
    "            parameters['b'+str(index)] = np.zeros(np.array(dimensions[index]))\n",
    "            parameters['w'+str(index)] = np.random.randn(dimensions[index],dimensions[index-1]) * 0.01\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正向传播\n",
    "\n",
    "def forward(dimensions,activation,X_img_array,parameters):\n",
    "    cache = {}\n",
    "    for i in range(len(dimensions)):\n",
    "        if i == 0:\n",
    "            cache['z'+str(i)] = X_img_array + parameters['b'+str(i)]\n",
    "        else:\n",
    "            cache['z'+str(i)] = np.dot(parameters['w'+str(i)],cache['a'+str(i-1)]) + parameters['b'+str(i)]\n",
    "        cache['a'+str(i)] = activation[i](cache['z'+str(i)])\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降\n",
    "\n",
    "def grad_parameters(dimensions,activation,differentiator,cache,Y_lab,parameters):\n",
    "    y_pre = cache['a'+str(len(dimensions)-1)]\n",
    "    y = Y_lab\n",
    "    j_y = ((1 - y) / (1 - y_pre)) - (y / y_pre)\n",
    "    grad_parameters = {}\n",
    "    layer_num = len(cache)//2 - 1\n",
    "    for i in range(layer_num,-1,-1):\n",
    "        if i == layer_num:\n",
    "            grad_parameters['db'+str(i)] = j_y * differentiator[activation[i]](cache['z'+str(i)])\n",
    "        else:\n",
    "            grad_parameters['db'+str(i)] = j_y * differentiator[activation[i]](cache['z'+str(i)]) *\\\n",
    "                np.dot(parameters['w'+str(i+1)].T,grad_parameters['db'+str(i+1)])\n",
    "        if i != 0:\n",
    "            grad_parameters['dw'+str(i)] = j_y * np.outer(grad_parameters['db'+str(i)],cache['a'+str(i-1)])\n",
    "    return grad_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失计算\n",
    "\n",
    "def J (y,y_pre):\n",
    "    return -(y * np.log(y_pre) + (1 - y) * np.log(1 - y_pre))\n",
    "\n",
    "def one_loss(dimensions,cache,Y_lab):\n",
    "    y_pre = cache['a'+str(len(dimensions)-1)]\n",
    "    y = Y_lab\n",
    "    j = -(y * np.log(y_pre) + (1 - y) * np.log(1 - y_pre))\n",
    "    return j\n",
    "\n",
    "def avg_loss(dimensions,activation,X_train,Y_train,parameters):\n",
    "    avg_loss = 0\n",
    "    for i in range(len(X_train)):\n",
    "        cache =forward(dimensions,activation,X_train[i],parameters)\n",
    "        avg_loss += one_loss(dimensions,cache,Y_train[i])\n",
    "    avg_loss /= len(X_train)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 梯度下降验证\n",
    "\n",
    "# def db_check (b,db,dimensions,activation,X_img_array,Y_lab,parameters,theta = 0.00001):\n",
    "#     y = Y_lab\n",
    "#     parameters_min = parameters\n",
    "#     parameters_max = parameters\n",
    "#     parameters_min[str(b)] -= theta\n",
    "#     parameters_max[str(b)] += theta\n",
    "#     cache_min = forward(dimensions,activation,X_img_array,parameters_min)\n",
    "#     cache_max = forward(dimensions,activation,X_img_array,parameters_max)\n",
    "#     y_min = cache_min['a'+str(len(dimensions)-1)]\n",
    "#     y_max = cache_max['a'+str(len(dimensions)-1)]\n",
    "#     db_pre = (J(y,y_max) - J(y,y_min)) / (theta * 2)\n",
    "#     db = db.reshape(db[0],1)\n",
    "#     return db - db_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions = [64*64,20,5,1]\n",
    "# activation = [relu,relu,relu,sigmoid]\n",
    "# differentiator = {sigmoid:sigmoid_back,relu:relu_back}\n",
    "\n",
    "# index = 4\n",
    "# parameters = init_parameters(dimensions)\n",
    "# grad_parameters = grad_parameters(dimensions,activation,differentiator,forward(dimensions,activation,X_train[index],parameters),Y_train[index],parameters)\n",
    "# print(parameters['b1'],grad_parameters['db1'],dimensions,activation,X_train[index],Y_train[index],parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数更新\n",
    "\n",
    "def update_parameters(grad_parameters,parameters,learn_rate):\n",
    "    up_parameters=copy.deepcopy(parameters)\n",
    "    for key in parameters.keys():\n",
    "        if key[0] == 'w':\n",
    "            up_parameters[key] = up_parameters[key] - learn_rate * grad_parameters['d'+key]\n",
    "        elif key[0] == 'b':\n",
    "            up_parameters[key] = up_parameters[key] - learn_rate * grad_parameters['d'+key]\n",
    "    return up_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分批量训练\n",
    "\n",
    "def train_batch(dimensions,activation,differentiator,X_train,Y_train,batch_size,current_batch,parameters):\n",
    "    cache = forward(dimensions,activation,X_train[current_batch * batch_size],parameters)\n",
    "    grad_accu =  grad_parameters(dimensions,activation,differentiator,cache,Y_train[current_batch * batch_size],parameters)\n",
    "    train_num = len(X_train)\n",
    "    if current_batch == train_num//batch_size:\n",
    "        batch_max = train_num - current_batch * batch_size\n",
    "    else:\n",
    "         batch_max = batch_size\n",
    "    for i in range(1,batch_max):\n",
    "        index = current_batch * batch_size + i\n",
    "        cache = forward(dimensions,activation,X_train[index],parameters)\n",
    "        for key in grad_accu.keys():\n",
    "            grad_accu[key] += grad_parameters(dimensions,activation,differentiator,cache,Y_train[index],parameters)[key]\n",
    "    for key in grad_accu.keys():\n",
    "            grad_accu[key] /= batch_max\n",
    "    return grad_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测函数\n",
    "\n",
    "def is_cat(dimensions,cache):\n",
    "    return cache['a'+str(len(dimensions)-1)] > 0.5\n",
    "\n",
    "def Test_accuracy(dimensions,activation,X_test,Y_test,parameters):\n",
    "   correct_num = 0\n",
    "   ac_rate = 0\n",
    "   all_num = len(X_test)\n",
    "   for i in range(0,all_num):\n",
    "       if is_cat(dimensions,forward(dimensions,activation,X_test[i],parameters)) == Y_test[i] :\n",
    "           correct_num += 1\n",
    "       else:\n",
    "           continue\n",
    "   ac_rate = correct_num / all_num * 100\n",
    "   return str(ac_rate) + ' %'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整模型\n",
    "\n",
    "def nn_module(dimensions,activation,differentiator,X_train,Y_train,batch_num,iter,learn_rate):\n",
    "    parameters = init_parameters(dimensions)\n",
    "    batch_size = len(X_train) // batch_num\n",
    "    i_num = 0\n",
    "    while(i_num < iter):\n",
    "        for current_batch in range(batch_num):\n",
    "            grad_parameters = train_batch(dimensions,activation,differentiator,X_train,Y_train,batch_size,current_batch,parameters)\n",
    "            parameters = update_parameters(grad_parameters,parameters,learn_rate)\n",
    "            print('第'+str(i_num)+'次迭代：')\n",
    "            print('模型准确率：',Test_accuracy(dimensions,activation,X_train,Y_train,parameters))\n",
    "            print('模型损失：',avg_loss(dimensions,activation,X_train,Y_train,parameters))\n",
    "        i_num += 1\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [64*64,55*55,44*44,33*33,28*28,16*16,9*9,5*5,3*3,1]\n",
    "activation = [relu,relu,relu,relu,relu,relu,relu,relu,relu,sigmoid]\n",
    "differentiator = {null:null,sigmoid:sigmoid_back,relu:relu_back}\n",
    "parameters = nn_module(dimensions,activation,differentiator,X_train,Y_train,batch_num=10,iter=20000,learn_rate=0.0075)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
