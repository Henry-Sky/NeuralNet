{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m train_dataset \u001b[39m=\u001b[39m h5py\u001b[39m.\u001b[39mFile(\u001b[39m'\u001b[39m\u001b[39mdatasets/train_catvnoncat.h5\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m test_dataset \u001b[39m=\u001b[39m h5py\u001b[39m.\u001b[39mFile(\u001b[39m'\u001b[39m\u001b[39mdatasets/test_catvnoncat.h5\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m X_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(train_dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtrain_set_x\u001b[39;49m\u001b[39m\"\u001b[39;49m][:])\u001b[39m.\u001b[39;49mcuda(\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     12\u001b[0m Y_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(train_dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain_set_y\u001b[39m\u001b[39m\"\u001b[39m][:])\u001b[39m.\u001b[39mcuda(\u001b[39m0\u001b[39m)\n\u001b[0;32m     14\u001b[0m X_train \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mreshape(\u001b[39m209\u001b[39m,\u001b[39m64\u001b[39m\u001b[39m*\u001b[39m\u001b[39m64\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcuda(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# 读取数据集\n",
    "train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "\n",
    "X_train = torch.tensor(train_dataset[\"train_set_x\"][:]).cuda(0)\n",
    "Y_train = torch.tensor(train_dataset[\"train_set_y\"][:]).cuda(0)\n",
    "\n",
    "X_train = X_train.reshape(209,64*64*3).cuda(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](img/%E7%A6%BB%E5%B7%AE%E5%BD%92%E4%B8%80%E5%8C%96.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 离差归一化\n",
    "\n",
    "def min_max_data(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "X_train = min_max_data(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数定义\n",
    "\n",
    "def relu(x):\n",
    "    return torch.relu(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数初始化\n",
    "\n",
    "def init_parameters(dimensions):\n",
    "    parameters = {}\n",
    "    layer_num = len(dimensions)\n",
    "    Xavier_rand = math.sqrt(6 / (dimensions[0]+dimensions[layer_num-1]))\n",
    "    for i in range(layer_num):\n",
    "        if i != 0:\n",
    "            parameters['w'+str(i)] = torch.Tensor(dimensions[i],dimensions[i-1]).uniform_(-Xavier_rand, Xavier_rand).cuda(0)\n",
    "            parameters['w'+str(i)].requires_grad = False\n",
    "        else:\n",
    "            pass\n",
    "        parameters['b'+str(i)] = torch.Tensor(dimensions[i]).zero_().cuda(0)\n",
    "        parameters['b'+str(i)].requires_grad = False\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失计算\n",
    "\n",
    "def loss_func(y,y_pre):\n",
    "    j = -(y * torch.log(y_pre) + (1 - y) * torch.log(1 - y_pre))\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播\n",
    "\n",
    "def forward_propagation(X_img,dimensions,activation,parameters):\n",
    "    for key in parameters.keys():\n",
    "        parameters[key] = parameters[key].detach()\n",
    "        parameters[key].requires_grad = True\n",
    "        parameters[key].retain_grad = True\n",
    "    cache = {}\n",
    "    layer_num = len(dimensions)\n",
    "    for i in range(layer_num):\n",
    "        if i == 0:\n",
    "            cache['z'+str(i)] = X_img + parameters['b'+str(i)]\n",
    "        else:\n",
    "            cache['z'+str(i)] = (parameters['w'+str(i)] @ cache['a'+str(i-1)]) + parameters['b'+str(i)]\n",
    "        cache['a'+str(i)] = activation[i](cache['z'+str(i)])\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向传播\n",
    "\n",
    "def backward_propagation(cache,Y_lab,parameters):\n",
    "    grad_parameters = {}\n",
    "    layer_num = len(cache)//2\n",
    "    y = Y_lab\n",
    "    y_pre = cache['a'+str(layer_num-1)]\n",
    "    j = loss_func(y,y_pre)\n",
    "    j.retain_graph = True\n",
    "    j.backward()\n",
    "    for i in range(layer_num):\n",
    "        grad_parameters['db'+str(i)] = parameters['b'+str(i)].grad\n",
    "        if i != 0:\n",
    "            grad_parameters['dw'+str(i)] = parameters['w'+str(i)].grad\n",
    "        else:\n",
    "            pass\n",
    "    j.retain_graph = False\n",
    "    for key in parameters.keys():\n",
    "        parameters[key].requires_grad = False\n",
    "        parameters[key].retain_grad = False\n",
    "    return grad_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新参数\n",
    "\n",
    "def update_parameters(parameters,grad_parameters,learn_rate = 0.01):\n",
    "    for key in parameters.keys():\n",
    "        parameters[key] = parameters[key] - grad_parameters['d'+key] * learn_rate\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测函数\n",
    "\n",
    "def predict(dimensions,activation,X_img,parameters,cat=False):\n",
    "    cache = {}\n",
    "    layer_num = len(dimensions)\n",
    "    for i in range(layer_num):\n",
    "        if i == 0:\n",
    "            cache['z'+str(i)] = X_img + parameters['b'+str(i)]\n",
    "        else:\n",
    "            cache['z'+str(i)] = (parameters['w'+str(i)] @ cache['a'+str(i-1)]) + parameters['b'+str(i)]\n",
    "        cache['a'+str(i)] = activation[i](cache['z'+str(i)])\n",
    "    if cat:\n",
    "        if cache['a'+str(layer_num-1)] > 0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return cache['a'+str(layer_num-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准确度评估\n",
    "\n",
    "def model_accuracy(X,Y,dimensions,activation,parameters):\n",
    "    img_num = torch.numel(Y)\n",
    "    ac_num = 0\n",
    "    for i in range(img_num):\n",
    "        y_pre = predict(dimensions,activation,X[i],parameters,cat=True)\n",
    "        y = Y[i]\n",
    "        if y_pre == y:\n",
    "            ac_num += 1\n",
    "        else:\n",
    "            continue\n",
    "    return (ac_num / img_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失评估\n",
    "\n",
    "def loss_accuracy(X,Y,dimensions,activation,parameters):\n",
    "    img_num = torch.numel(Y)\n",
    "    loss_sum = 0\n",
    "    for i in range(img_num):\n",
    "        y_pre = predict(dimensions,activation,X[i],parameters,cat=False)\n",
    "        y = Y[i]\n",
    "        loss_sum += loss_func(y,y_pre)\n",
    "    return (loss_sum / img_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions = [64*64*3,20,7,1]\n",
    "# activation = [relu,relu,relu,sigmoid]\n",
    "\n",
    "# parameters = init_parameters(dimensions)\n",
    "\n",
    "# index = 4\n",
    "# y = Y_train[index]\n",
    "# print(y)\n",
    "# y_pre = predict(dimensions,activation,X_train[index],parameters,cat=False)\n",
    "# print(y_pre)\n",
    "# loss_func(y,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions = [64*64*3,20,7,1]\n",
    "# activation = [relu,relu,relu,sigmoid]\n",
    "\n",
    "# index = 4\n",
    "\n",
    "# parameters = init_parameters(dimensions)\n",
    "# predict(dimensions,activation,X_train[index],parameters,cat=False)\n",
    "# print(loss_accuracy(X_train,Y_train,dimensions,activation,parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小批量训练\n",
    "\n",
    "def batch_train(current_batch,X_train,Y_train,batch_size,dimensions,activation,parameters):\n",
    "    num = len(Y_train)\n",
    "    if current_batch == num // batch_size:\n",
    "        batch_max = batch_size\n",
    "    else:\n",
    "        batch_max = num - current_batch * batch_size\n",
    "    index = current_batch * batch_size\n",
    "    cache = forward_propagation(X_train[index],dimensions,activation,parameters)\n",
    "    grad_parameters = backward_propagation(cache,Y_train[index],parameters)\n",
    "    grad_sum = copy.deepcopy(grad_parameters)\n",
    "    for i in range(batch_max):\n",
    "        index = current_batch * batch_size + i\n",
    "        cache = forward_propagation(X_train[index],dimensions,activation,parameters)\n",
    "        grad_parameters = backward_propagation(cache,Y_train[index],parameters)\n",
    "        for key in grad_sum.keys():\n",
    "            grad_sum[key] += grad_parameters[key]\n",
    "    for keys in grad_sum.keys():\n",
    "        grad_sum[keys] /= batch_max\n",
    "    return grad_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整模型\n",
    "\n",
    "def nn_model(X_train,Y_train,batch_size,dimensions,activation,iter,iter_print,learn_rate):\n",
    "    num = len(Y_train)\n",
    "    parameters = init_parameters(dimensions)\n",
    "    iter_num = 0\n",
    "    while(iter_num <= iter):\n",
    "        for i in range(num//batch_size):\n",
    "            grad_parameters = batch_train(i,X_train,Y_train,batch_size,dimensions,activation,parameters)\n",
    "            parameters = update_parameters(parameters,grad_parameters,learn_rate)\n",
    "        if iter_num % iter_print == 0:\n",
    "            print('第',iter_num,'次迭代')\n",
    "            print('模型准确率：',model_accuracy(X_train,Y_train,dimensions,activation,parameters)*100,' %')\n",
    "            print('模型损失：',loss_accuracy(X_train,Y_train,dimensions,activation,parameters))\n",
    "        iter_num += 1\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions = [64*64*3,20,7,1]\n",
    "# activation = [relu,relu,relu,sigmoid]\n",
    "\n",
    "# index = 4\n",
    "\n",
    "# parameters = init_parameters(dimensions)\n",
    "# cache = forward_propagation(X_train[index],dimensions,activation,parameters)\n",
    "# grad = backward_propagation(cache,Y_train[index],parameters)\n",
    "# print(grad['dw1'])\n",
    "# parameters = update_parameters(parameters,grad,0.01)\n",
    "# print('-----------------------------------------------------------')\n",
    "# print(loss_accuracy(X_train,Y_train,dimensions,activation,parameters))\n",
    "# print('-----------------------------------------------------------')\n",
    "\n",
    "# index = 3\n",
    "# cache = forward_propagation(X_train[index],dimensions,activation,parameters)\n",
    "# grad = backward_propagation(cache,Y_train[index],parameters)\n",
    "# print(grad['dw1'])\n",
    "# parameters = update_parameters(parameters,grad,0.01)\n",
    "\n",
    "# print('-----------------------------------------------------------')\n",
    "# print(loss_accuracy(X_train,Y_train,dimensions,activation,parameters))\n",
    "# print('-----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache = forward_propagation(X_train[index],dimensions,activation,parameters)\n",
    "# grad = backward_propagation(cache,Y_train[index],parameters)\n",
    "# print(grad['dw1'])\n",
    "# # parameters = update_parameters(parameters,grad,0.01)\n",
    "# print('-----------------------------------------------------------------------------------')\n",
    "# print(parameters['w1'])\n",
    "# parameters = update_parameters(parameters,grad,0.01)\n",
    "# print('-----------------------------------------------------------------------------------')\n",
    "# print(parameters['w1'])\n",
    "# cache = forward_propagation(X_train[index],dimensions,activation,parameters)\n",
    "# grad = backward_propagation(cache,Y_train[index],parameters)\n",
    "# # print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grad['dw1'])\n",
    "\n",
    "# iter = 2000\n",
    "# while(iter):\n",
    "#     cache = forward_propagation(X_train[index],dimensions,activation,parameters)\n",
    "#     grad = backward_propagation(cache,Y_train[index],parameters)\n",
    "#     parameters = update_parameters(parameters,grad,0.001)\n",
    "#     iter = iter - 1\n",
    "\n",
    "# print(grad['dw1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 次迭代\n",
      "模型准确率： 65.55023923444976  %\n",
      "模型损失： tensor([0.6552], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemv(handle, op, m, n, &alpha, a, lda, x, incx, &beta, y, incy)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m dimensions \u001b[39m=\u001b[39m [\u001b[39m64\u001b[39m\u001b[39m*\u001b[39m\u001b[39m64\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m,\u001b[39m45\u001b[39m\u001b[39m*\u001b[39m\u001b[39m45\u001b[39m,\u001b[39m5\u001b[39m\u001b[39m*\u001b[39m\u001b[39m5\u001b[39m,\u001b[39m7\u001b[39m,\u001b[39m1\u001b[39m]\n\u001b[0;32m      2\u001b[0m activation \u001b[39m=\u001b[39m [relu,relu,relu,relu,sigmoid]\n\u001b[1;32m----> 4\u001b[0m parameters \u001b[39m=\u001b[39m nn_model(X_train,Y_train,\u001b[39m1\u001b[39;49m,dimensions,activation,\u001b[39m2000\u001b[39;49m,\u001b[39m20\u001b[39;49m,\u001b[39m0.01\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[34], line 9\u001b[0m, in \u001b[0;36mnn_model\u001b[1;34m(X_train, Y_train, batch_size, dimensions, activation, iter, iter_print, learn_rate)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m(iter_num \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39miter\u001b[39m):\n\u001b[0;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mbatch_size):\n\u001b[1;32m----> 9\u001b[0m         grad_parameters \u001b[39m=\u001b[39m batch_train(i,X_train,Y_train,batch_size,dimensions,activation,parameters)\n\u001b[0;32m     10\u001b[0m         parameters \u001b[39m=\u001b[39m update_parameters(parameters,grad_parameters,learn_rate)\n\u001b[0;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m iter_num \u001b[39m%\u001b[39m iter_print \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[33], line 15\u001b[0m, in \u001b[0;36mbatch_train\u001b[1;34m(current_batch, X_train, Y_train, batch_size, dimensions, activation, parameters)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_max):\n\u001b[0;32m     14\u001b[0m     index \u001b[39m=\u001b[39m current_batch \u001b[39m*\u001b[39m batch_size \u001b[39m+\u001b[39m i\n\u001b[1;32m---> 15\u001b[0m     cache \u001b[39m=\u001b[39m forward_propagation(X_train[index],dimensions,activation,parameters)\n\u001b[0;32m     16\u001b[0m     grad_parameters \u001b[39m=\u001b[39m backward_propagation(cache,Y_train[index],parameters)\n\u001b[0;32m     17\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m grad_sum\u001b[39m.\u001b[39mkeys():\n",
      "Cell \u001b[1;32mIn[25], line 14\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(X_img, dimensions, activation, parameters)\u001b[0m\n\u001b[0;32m     12\u001b[0m         cache[\u001b[39m'\u001b[39m\u001b[39mz\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)] \u001b[39m=\u001b[39m X_img \u001b[39m+\u001b[39m parameters[\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)]\n\u001b[0;32m     13\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m         cache[\u001b[39m'\u001b[39m\u001b[39mz\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)] \u001b[39m=\u001b[39m (parameters[\u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39mstr\u001b[39;49m(i)] \u001b[39m@\u001b[39;49m cache[\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39mstr\u001b[39;49m(i\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)]) \u001b[39m+\u001b[39m parameters[\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)]\n\u001b[0;32m     15\u001b[0m     cache[\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)] \u001b[39m=\u001b[39m activation[i](cache[\u001b[39m'\u001b[39m\u001b[39mz\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)])\n\u001b[0;32m     16\u001b[0m \u001b[39mreturn\u001b[39;00m cache\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemv(handle, op, m, n, &alpha, a, lda, x, incx, &beta, y, incy)`"
     ]
    }
   ],
   "source": [
    "dimensions = [64*64*3,45*45,5*5,7,1]\n",
    "activation = [relu,relu,relu,relu,sigmoid]\n",
    "\n",
    "parameters = nn_model(X_train,Y_train,5,dimensions,activation,2000,5,0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
